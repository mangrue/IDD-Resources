{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mangrue/IDD-Resources/blob/main/IDD_Datasets_DataPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeaYDCuSzZdv"
      },
      "source": [
        "#**Setting up environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9dasqVDUSwcF"
      },
      "outputs": [],
      "source": [
        "# Install specific libraries\n",
        "#! pip install torch\n",
        "#! pip install transformers\n",
        "#! pip install pycaret\n",
        "#! pip install pandas\n",
        "#! pip install numpy\n",
        "#! pip install pycaret\n",
        "#! pip install matplotlib\n",
        "#! pip install -U scikit-learn\n",
        "#! pip install transformers==2.8.0\n",
        "#!pip install --upgrade huggingface_hub\n",
        "!pip install evaluate -q\n",
        "!pip install datasets -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import pycaret\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Set Working Directory - if working on Google Drive\n",
        "# Mount Google Drive - applicable, if working on Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#%cd /content/drive/MyDrive #/Colab_Notebooks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEZ-wTB7zoho"
      },
      "source": [
        "#**Load and prepare dataset(s) - generating 'final' versions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z-pnVbibwdY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#ISOT\n",
        "#==============================================================================================================================\n",
        "\n",
        "# Load dataset\n",
        "true_data = pd.read_csv('/content/drive/MyDrive/DATASETS/ISOT_News_dataset/True.csv')\n",
        "fake_data = pd.read_csv('/content/drive/MyDrive/DATASETS/ISOT_News_dataset/Fake.csv')\n",
        "\n",
        "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
        "true_data['label'] = ['True']*len(true_data)\n",
        "fake_data['label'] = ['False']*len(fake_data)\n",
        "\n",
        "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
        "data = true_data._append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['True']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "#print(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#use title and text\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "#use only title\n",
        "#df['text'] = df['title']\n",
        "#finaldf = df[['text', 'label']]\n",
        "\n",
        "#use only text\n",
        "#finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/ISOT_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "#finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/ISOT_final_only_title.csv\", sep=',', index=False, encoding='utf-8')\n",
        "#finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/ISOT_final_only_text.csv\", sep=',', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#NELA-GT-2018\n",
        "#==============================================================================================================================\n",
        "\n",
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "import sqlite3\n",
        "#import create_engine from sqlalchemy\n",
        "#from sqlalchemy import create_engine\n",
        "#import pandas\n",
        "import pandas as pd\n",
        "\n",
        "#read database using create_engine\n",
        "dbfile = '/content/drive/MyDrive/DATASETS/NELA-GT-2018/nela-gt-2018.db'\n",
        "con = sqlite3.connect(dbfile)\n",
        "\n",
        "# creating cursor\n",
        "#cur = con.cursor()\n",
        "\n",
        "# reading all table names\n",
        "#table_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
        "# here is you table list\n",
        "#print(table_list)\n",
        "\n",
        "#takes about 5-6 mins due to large size of dataset\n",
        "df = pd.read_sql_query(\"SELECT source, title, content FROM newsdata LIMIT 162411\", con) #LIMIT to 20% of overall dataset (812 057)\n",
        "#print(df)\n",
        "\n",
        "# Be sure to close the connection\n",
        "con.close()\n",
        "\n",
        "df_labels = pd.read_csv(\"/content/drive/MyDrive/DATASETS/NELA-GT-2018/labels.csv\", sep = ',', header=None)\n",
        "df_labels = df_labels.drop(df_labels.index[0])\n",
        "df_labels.columns = ['source', 'label']\n",
        "#0 - reliable (TRUE), 1 - mixed (MIXED), 2 - unreliable (FALSE)\n",
        "#df_labels = df_labels.drop(df_labels[df_labels['label'] == '1'].index)\n",
        "df_labels.loc[df_labels.label == \"0\", 'label'] = \"TRUE\"\n",
        "df_labels.loc[df_labels.label == \"1\", 'label'] = \"MIXED\"\n",
        "df_labels.loc[df_labels.label == \"2\", 'label'] = \"FALSE\"\n",
        "#print(df_labels)\n",
        "\n",
        "df_merged = df.merge(df_labels, on = 'source', how = 'outer')\n",
        "df_merged = df_merged.drop('source', axis=1)\n",
        "df_merged = df_merged.drop(df_merged[df_merged['label'] == 'MIXED'].index)\n",
        "df_merged = df_merged.rename(columns={\"content\": \"text\"})\n",
        "#print(df_merged)\n",
        "\n",
        "data = df_merged\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['TRUE']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "#print(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "#Check if None values exist in rows\n",
        "#null_mask = finaldf.isnull().any(axis=1)\n",
        "#null_rows = finaldf[null_mask]\n",
        "#print(null_rows)\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2018_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "\n",
        "#Remove 'None' again\n",
        "dataset = load_dataset(\"csv\", header=0, data_files=\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2018_final.csv\")\n",
        "dataset.set_format(type='pandas')\n",
        "df = dataset['train'][:]\n",
        "df = df[~df.isnull().any(axis=1)]\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2018_final.csv\", sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "sh4FPF-xF3K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C--MbqllcUeY"
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#NELA-GT-2022\n",
        "#==============================================================================================================================\n",
        "\n",
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "import sqlite3\n",
        "#import create_engine from sqlalchemy\n",
        "#from sqlalchemy import create_engine\n",
        "#import pandas\n",
        "import pandas as pd\n",
        "\n",
        "#read database using create_engine\n",
        "dbfile = '/content/drive/MyDrive/DATASETS/NELA-GT-2022/nela-gt-2022.db'\n",
        "con = sqlite3.connect(dbfile)\n",
        "\n",
        "# creating cursor\n",
        "#cur = con.cursor()\n",
        "\n",
        "# reading all table names\n",
        "#table_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n",
        "# here is you table list\n",
        "#print(table_list)\n",
        "\n",
        "#takes about 5-6 mins due to large size of dataset\n",
        "df = pd.read_sql_query(\"SELECT source, title, content FROM newsdata LIMIT 177866\", con) #LIMIT to 10% of overall dataset (1 778 361)\n",
        "#print(df)\n",
        "\n",
        "# Be sure to close the connection\n",
        "con.close()\n",
        "\n",
        "df_labels = pd.read_csv(\"/content/drive/MyDrive/DATASETS/NELA-GT-2022/labels.csv\", sep = ',', header=None)\n",
        "df_labels = df_labels.drop(df_labels.index[0])\n",
        "df_labels.columns = ['source', 'label']\n",
        "#0 - reliable (TRUE), 1 - mixed (MIXED), 2 - unreliable (FALSE)\n",
        "#df_labels = df_labels.drop(df_labels[df_labels['label'] == '1'].index)\n",
        "df_labels.loc[df_labels.label == \"0\", 'label'] = \"TRUE\"\n",
        "df_labels.loc[df_labels.label == \"1\", 'label'] = \"MIXED\"\n",
        "df_labels.loc[df_labels.label == \"2\", 'label'] = \"FALSE\"\n",
        "#print(df_labels)\n",
        "\n",
        "df_merged = df.merge(df_labels, on = 'source', how = 'outer')\n",
        "df_merged = df_merged.drop('source', axis=1)\n",
        "df_merged = df_merged.drop(df_merged[df_merged['label'] == 'MIXED'].index)\n",
        "df_merged = df_merged.rename(columns={\"content\": \"text\"})\n",
        "#print(df_merged)\n",
        "\n",
        "data = df_merged\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['TRUE']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "#print(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "#Check if None values exist in rows\n",
        "#null_mask = finaldf.isnull().any(axis=1)\n",
        "#null_rows = finaldf[null_mask]\n",
        "#print(null_rows)\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2022_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "\n",
        "#Remove 'None' again\n",
        "dataset = load_dataset(\"csv\", header=0, data_files=\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2022_final.csv\")\n",
        "dataset.set_format(type='pandas')\n",
        "df = dataset['train'][:]\n",
        "df = df[~df.isnull().any(axis=1)]\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/NELA-GT-2022_final.csv\", sep=',', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "boECX0hicVZp"
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#BuzzFeed - Store as CSV\n",
        "#==============================================================================================================================\n",
        "\n",
        "#Load dataset\n",
        "#data = pd.read_xml('/content/drive/MyDrive/DATASETS/BuzzFeed-WebisFakeNewsCorpus2016/articls/articles')\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as et\n",
        "df_cols= ['mainText', 'veracity']\n",
        "df= pd.DataFrame([], columns=df_cols)\n",
        "path='/content/drive/MyDrive/DATASETS/BuzzFeed-WebisFakeNewsCorpus2016/articles/articles/'\n",
        "\n",
        "i = 0\n",
        "df_cols= ['title','text','label']\n",
        "df_final = pd.DataFrame([], columns=df_cols)\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "    if not filename.endswith('.xml'): continue\n",
        "    fullname = os.path.join(path, filename)\n",
        "    xtree = et.parse(fullname)\n",
        "    xroot = xtree.getroot()\n",
        "    rows = []\n",
        "    for record in xroot:\n",
        "      mtitle = xroot.find('title').text\n",
        "      mtext = xroot.find('mainText').text\n",
        "      vlabel = xroot.find('veracity').text\n",
        "      rows.append({\"title\": mtitle,\"text\": mtext,\"label\": vlabel})\n",
        "    df_temp = pd.DataFrame(rows, columns = df_cols)\n",
        "    df_final = pd.concat([df_final,df_temp])\n",
        "    #df.to_csv('/content/drive/MyDrive/DATASETS/BuzzFeed-WebisFakeNewsCorpus2016/articles/articles_csv/'+ str(i) + '.csv', index=False)\n",
        "    #i+=1\n",
        "\n",
        "print(df_final)\n",
        "df_final = df_final.drop_duplicates(subset=['text'])\n",
        "print(df_final)\n",
        "df_final.to_csv('/content/drive/MyDrive/DATASETS/BuzzFeed-WebisFakeNewsCorpus2016/BuzzFeed-WebisFakeNewsCorpus2016.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHOgxxL_HzJR"
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#BuzzFeed\n",
        "#==============================================================================================================================\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATASETS/BuzzFeed-WebisFakeNewsCorpus2016/BuzzFeed-WebisFakeNewsCorpus2016.csv\", sep = ',', header=None)\n",
        "data = data.drop(data.index[0])\n",
        "data.columns = ['title', 'text', 'label']\n",
        "\n",
        "#changing mostly false and no factual content labels to FALSE\n",
        "data.loc[data.label == \"mostly false\", 'label'] = \"FALSE\"\n",
        "data.loc[data.label == \"no factual content\", 'label'] = \"FALSE\"\n",
        "#delete column with mixture of true and false (potentially this could be good data for a category 'mixed')\n",
        "data = data.drop(data[data['label'] == 'mixture of true and false'].index)\n",
        "#changing mostly-true label to TRUE\n",
        "data.loc[data.label == \"mostly true\", 'label'] = \"TRUE\"\n",
        "\n",
        "#print(data)\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['TRUE']  #dtype=int results in 0/1 not True/False\n",
        "\n",
        "#print(data)\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "#print(label_size)\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Change capitalized titles to sentence cased (save as different csv)\n",
        "df['title'] = df['title'].str.capitalize()\n",
        "\n",
        "#merge title and text\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "#only text\n",
        "#df['text'] = df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "#finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/BuzzFeed-2016_final_only_text.csv\", sep=',', index=False, encoding='utf-8')\n",
        "#finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/BuzzFeed-2016_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/BuzzFeed-2016_final_titles_sentence_case.csv\", sep=',', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqlQFdYKXL-e"
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#LIAR\n",
        "#==============================================================================================================================\n",
        "#Read dataset (as dataframe)\n",
        "test_dataset = pd.read_csv(\"/content/drive/MyDrive/DATASETS/liar_dataset/test.tsv\", sep = '\\t', header=None)\n",
        "train_dataset = pd.read_csv(\"/content/drive/MyDrive/DATASETS/liar_dataset/train.tsv\", sep = '\\t', header=None)\n",
        "validate_dataset = pd.read_csv(\"/content/drive/MyDrive/DATASETS/liar_dataset/valid.tsv\", sep = '\\t', header=None)\n",
        "#Merge datasets to one\n",
        "data = test_dataset._append(train_dataset).sample(frac=1).reset_index().drop(columns=['index'])\n",
        "data = data._append(validate_dataset).sample(frac=1).reset_index().drop(columns=['index'])\n",
        "#add labels\n",
        "data.columns = ['ID','label', 'text', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', 'barely true counts', 'false counts', 'half true counts', 'mostly true counts', 'pants on fire counts', 'context']\n",
        "#print(data)                  #instead of 'statement' use label 'text'\n",
        "\n",
        "#changing false and pants-fire labels to FALSE\n",
        "data.loc[data.label == \"pants-fire\", 'label'] = \"FALSE\"\n",
        "data.loc[data.label == \"false\", 'label'] = \"FALSE\"\n",
        "#delete column with half-true or barely-true (potentially this could be good data for a category 'mixed')\n",
        "#data.loc[data.label == \"half-true\", 'label'] = \"FALSE\"\n",
        "#data.loc[data.label == \"barely-true\", 'label'] = \"FALSE\"\n",
        "data = data.drop(data[data['label'] == 'half-true'].index)\n",
        "data = data.drop(data[data['label'] == 'barely-true'].index)\n",
        "#changing true and mostly-true labels to TRUE\n",
        "data.loc[data.label == \"true\", 'label'] = \"TRUE\"\n",
        "data.loc[data.label == \"mostly-true\", 'label'] = \"TRUE\"\n",
        "\n",
        "#data.rename(columns = {'label':'target'}, inplace = True)\n",
        "\n",
        "#random mixing of dataframe rows\n",
        "#data = data. sample(frac=1)\n",
        "#print(data)\n",
        "\n",
        "# Label column is made of string values True/False, let's change it to numbers 0/1 (Real or TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['TRUE']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "finaldf = df[[\"text\", \"label\"]]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/LIAR_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi-81ubKvGn_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#==============================================================================================================================\n",
        "#WELFAKE\n",
        "#==============================================================================================================================\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/DATASETS/WELFake/WELFake_Dataset.csv\", sep = ',')\n",
        "print(data)\n",
        "\n",
        "#(0 = fake and 1 = real)\n",
        "data.loc[data.label == 1, 'label'] = \"TRUE\"\n",
        "data.loc[data.label == 0, 'label'] = \"FALSE\"\n",
        "\n",
        "# Label column is made of string values True/False, let's change it to numbers 0/1 (Real or TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['TRUE']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "finaldf = finaldf.drop_duplicates()\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/WELFake_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "\n",
        "print(finaldf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#clmentbisaillon - fake-and-real-news-dataset / ISOT\n",
        "#==============================================================================================================================\n",
        "\n",
        "# Load dataset\n",
        "true_data = pd.read_csv('/content/drive/MyDrive/DATASETS/clmentbisaillon - fake-and-real-news-dataset - nlp fake news detection dataset/True.csv')\n",
        "fake_data = pd.read_csv('/content/drive/MyDrive/DATASETS/clmentbisaillon - fake-and-real-news-dataset - nlp fake news detection dataset/Fake.csv')\n",
        "\n",
        "# Generate labels True/Fake under new Target Column in 'true_data' and 'fake_data'\n",
        "true_data['label'] = ['True']*len(true_data)\n",
        "fake_data['label'] = ['False']*len(fake_data)\n",
        "\n",
        "# Merge 'true_data' and 'fake_data', by random mixing into a single df called 'data'\n",
        "data = true_data._append(fake_data).sample(frac=1).reset_index().drop(columns=['index'])\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "data['label'] = pd.get_dummies(data.label)['True']\n",
        "\n",
        "# Checking if our data is well balanced\n",
        "label_size = [data['label'].sum(),len(data['label'])-data['label'].sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['navy','firebrick'],startangle=90,shadow=True,labels=['True','False'],autopct='%1.1f%%')\n",
        "plt.show()\n",
        "\n",
        "#print(data)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['text'] = df['title'] + '\\n\\n' + df['text']\n",
        "finaldf = df[['text', 'label']]\n",
        "\n",
        "finaldf = finaldf.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "finaldf.dropna(subset=['text'], inplace=False)\n",
        "finaldf.dropna(subset=['label'], inplace=False)\n",
        "# Select Rows without NaN Values in Specific Column\n",
        "#finaldf = finaldf[~finaldf['text'].isna()]\n",
        "# Select Rows without NaN Values in All Columns\n",
        "finaldf = finaldf[~finaldf.isnull().any(axis=1)]\n",
        "\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/clmentbisaillon_final.csv\", sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wsaOtPLy4pFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#Fighting an Infodemic: COVID-19 Fake News Dataset\n",
        "#==============================================================================================================================\n",
        "\n",
        "#login to hugging face\n",
        "!huggingface-cli login --token #hugging face token #--add-to-git-credential\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"nanyy1025/covid_fake_news\")\n",
        "\n",
        "print(ds)\n",
        "\n",
        "ds = ds.rename_column(\"tweet\", \"text\")\n",
        "#label = string real vs fake\n",
        "#adjust to true=1 and false=0\n",
        "\n",
        "finaldf = ds['train'].to_pandas()\n",
        "\n",
        "finaldf.loc[finaldf.label == \"real\", 'label'] = \"TRUE\"\n",
        "finaldf.loc[finaldf.label == \"fake\", 'label'] = \"FALSE\"\n",
        "\n",
        "# Label column is made of string values True/Fake, let's change it to numbers 0/1 (TRUE=1)\n",
        "finaldf['label'] = pd.get_dummies(finaldf.label)['TRUE']  #dtype=int results in 0/1 not True/False\n",
        "\n",
        "print(finaldf)\n",
        "#finaldf.sample(n=100)\n",
        "\n",
        "#ds['train'].to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/COVID19-fake-news_final.csv\", sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "adffZctMQPBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#jy46604790 - fake dataset with EU news // EU vs Disinfo data\n",
        "#==============================================================================================================================\n",
        "\n",
        "#login to hugging face\n",
        "!huggingface-cli login --token #hugging face token #--add-to-git-credential\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"winterForestStump/fake-news-detector-euvsdisinfo\")\n",
        "\n",
        "#print(ds)\n",
        "\n",
        "ds = ds.rename_column(\"Title\", \"title\")\n",
        "ds = ds.rename_column(\"Summary\", \"text\")\n",
        "\n",
        "ds = ds['train'].add_column(name=\"label\", column=[i for i in range(len(ds['train']))])\n",
        "\n",
        "finaldf = ds.to_pandas()\n",
        "\n",
        "finaldf = finaldf.assign(label=0) #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#print(finaldf)\n",
        "#finaldf.sample(n=100)\n",
        "\n",
        "#ds['train'].to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',', index=False, encoding='utf-8')\n",
        "finaldf.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "vb5by4IQm7JO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Inspect data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep = ',')\n",
        "\n",
        "#print all rows to inspect\n",
        "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
        "    #print(df)\n",
        "\n",
        "print(df)\n",
        "\n",
        "#print(\"Rows:\", len(df.index))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aPVWeG9Li5L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean europeannewsroom data**"
      ],
      "metadata": {
        "id": "Gee0s64Kpgoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_dump.csv\", sep = ';', header=None, names=['Date', 'Category', 'Title', 'Shorttext', 'Longtext'])\n",
        "#print(df)\n",
        "\n",
        "df.drop(df.columns[[0]], axis=1, inplace=True) #drop Date\n",
        "df.drop(df.columns[[0]], axis=1, inplace=True) #drop Category\n",
        "#print(df)\n",
        "\n",
        "#Remove html from content/Longtext\n",
        "for index, row in df.iterrows():\n",
        "    row['Longtext'] = BeautifulSoup(str(row['Longtext'])).get_text()\n",
        "\n",
        "df['Longtext'] = df['Shorttext'] + df['Longtext']\n",
        "df.drop(df.columns[[1]], axis=1, inplace=True) #drop Shorttext\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (for this dataset all true so 1)\n",
        "\n",
        "df.rename(columns={'Title': 'title', 'Longtext': 'text'}, inplace=True)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save enr_dump_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final_uncleaned.csv\", sep = ',', index = False)\n",
        "\n",
        "#if necessary manually adjust any remaining coding errors or html and save as cleaned version = enr_final"
      ],
      "metadata": {
        "id": "OEzh7D2MplPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select and clean EMNAD data**"
      ],
      "metadata": {
        "id": "jch25o87p_ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (for this dataset all true so 1)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "nRxnJAYUqFBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each MS:**"
      ],
      "metadata": {
        "id": "i8qDpfSd5Kf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Belgium_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Belgium_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "gJgEI-N74zwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Cyprus_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Cyprus_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "cD2xVE005A1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Ireland_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Ireland_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "SCIQoiHU5BD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Malta_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Malta_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "2aj0tWwa5BK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/United_Kingdom_sample.csv\", sep = ',')\n",
        "\n",
        "df.rename(columns = {'Article Body':'text'}, inplace = True)\n",
        "df.rename(columns = {'Article Title':'title'}, inplace = True)\n",
        "df = df[['text', 'title']]\n",
        "df = df.reindex(columns=['title', 'text'])\n",
        "\n",
        "#add True label\n",
        "df.insert(2, \"label\", 1)  #Real or True = 1, False = 0 (all False for this dataset, no True available)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "#save EMNAD_sample_final\n",
        "df.to_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/United_Kingdom_sample_final.csv\", sep = ',', index = False)"
      ],
      "metadata": {
        "id": "0Faat3mO5DOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build final merged Datasets (enr_final.csv, EMNAD_sample_final.csv, and jy46604790_fake_dataset_final.csv)**"
      ],
      "metadata": {
        "id": "S3V95LtUbiN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sizes:**\n",
        "\n",
        "***enr_final.csv:*** 536 (uncleaned) - 510 (cleaned = final)\n",
        "\n",
        "***EMNAD_sample_final.csv:*** 50,000\n",
        "\n",
        "***jy46604790_fake_dataset_final.csv:*** 9,965"
      ],
      "metadata": {
        "id": "nUkJhoVnFKJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "df_EMNAD = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample_final.csv\", sep = ',')\n",
        "print(df_EMNAD)"
      ],
      "metadata": {
        "id": "4NJLr2p-8f4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 1 ([9,455 EMNAD articles and 510 europeannewsroom articles] + 9,965 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_enr = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final.csv\", sep = ',')\n",
        "df_EMNAD = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_enr = df_enr.sample(n=510, random_state=42)\n",
        "df_EMNAD = df_EMNAD.sample(n=9455, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=9965, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_enr, df_EMNAD], ignore_index=True)\n",
        "df_mix = pd.concat([df_mix, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_1_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "BnwE6hiQbgKY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 2 ([510 EMNAD articles and 510 europeannewsroom articles] + 1,020 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_enr = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final.csv\", sep = ',')\n",
        "df_EMNAD = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_enr = df_enr.sample(n=510, random_state=42)\n",
        "df_EMNAD = df_EMNAD.sample(n=510, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=1020, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_enr, df_EMNAD], ignore_index=True)\n",
        "df_mix = pd.concat([df_mix, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_2_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "3dfJr-XNhE08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 3 EMNAD (9,965 EMNAD articles + 9,965 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_EMNAD = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/EMNAD_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_EMNAD = df_EMNAD.sample(n=9965, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=9965, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_EMNAD, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_3_EMNAD_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "Eh096jgno1hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 4 enr (510 europeannewsroom articles + 510 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_enr = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_enr = df_enr.sample(n=510, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=510, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_enr, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_4_enr_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "JKVEig6K8l2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 5 enr & Belgium (510 europeannewsroom articles + 9,455 Belgium articles + 9,965 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_enr = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final.csv\", sep = ',')\n",
        "df_Belgium = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Belgium_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_enr = df_enr.sample(n=510, random_state=42)\n",
        "df_Belgium = df_Belgium.sample(n=9455, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=9965, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_enr, df_Belgium], ignore_index=True)\n",
        "df_mix = pd.concat([df_mix, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_5_enr_Belgium_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "FzPhJYRi89Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU mix 6 enr & Belgium (510 europeannewsroom articles + 510 Belgium articles + 1,020 EUvsDisinfo articles)\n",
        "#==============================================================================================================================\n",
        "df_enr = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_dpa_europeannewsroom_com/enr_final.csv\", sep = ',')\n",
        "df_Belgium = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Belgium_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_enr = df_enr.sample(n=510, random_state=42)\n",
        "df_Belgium = df_Belgium.sample(n=510, random_state=42)\n",
        "df_EUvsDis = df_EUvsDis.sample(n=1020, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_mix = pd.concat([df_enr, df_Belgium], ignore_index=True)\n",
        "df_mix = pd.concat([df_mix, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_mix['text'] = df_mix['title'] + '\\n\\n' + df_mix['text']\n",
        "\n",
        "#save\n",
        "df_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_6_enr_Belgium_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "W-mPJh-r-Tbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Each MS:**"
      ],
      "metadata": {
        "id": "LJnMWsdC9Rml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================================================================\n",
        "#EU MS (Belgium, Cyprus, Ireland, Malta, United Kingdom) + EUvsDisinfo (each 9,965:9,965 EUvsDisinfo, mix with same shuffle sample of EUvsDisinfo - enables a better comparison)\n",
        "#==============================================================================================================================\n",
        "df_Belgium = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Belgium_sample_final.csv\", sep = ',')\n",
        "df_Cyprus = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Cyprus_sample_final.csv\", sep = ',')\n",
        "df_Ireland = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Ireland_sample_final.csv\", sep = ',')\n",
        "df_Malta = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/Malta_sample_final.csv\", sep = ',')\n",
        "df_United_Kingdom = pd.read_csv(\"/content/drive/MyDrive/DATASETS/EU_European Multilingual News Articles Dataset with Topic Annotation/United_Kingdom_sample_final.csv\", sep = ',')\n",
        "df_EUvsDis = pd.read_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/jy46604790_fake_dataset_EU_final.csv\", sep=',')\n",
        "\n",
        "#random selection of portions\n",
        "df_Belgium = df_Belgium.sample(n=9965, random_state=42)\n",
        "df_Cyprus = df_Cyprus.sample(n=9965, random_state=42)\n",
        "df_Ireland = df_Ireland.sample(n=9965, random_state=42)\n",
        "df_Malta = df_Malta.sample(n=9965, random_state=42)\n",
        "df_United_Kingdom = df_United_Kingdom.sample(n=9965, random_state=42)\n",
        "\n",
        "df_EUvsDis = df_EUvsDis.sample(n=9965, random_state=42)\n",
        "\n",
        "#merge dfs\n",
        "df_Belgium_mix = pd.concat([df_Belgium, df_EUvsDis], ignore_index=True)\n",
        "df_Cyprus_mix = pd.concat([df_Cyprus, df_EUvsDis], ignore_index=True)\n",
        "df_Ireland_mix = pd.concat([df_Ireland, df_EUvsDis], ignore_index=True)\n",
        "df_Malta_mix = pd.concat([df_Malta, df_EUvsDis], ignore_index=True)\n",
        "df_United_Kingdom_mix = pd.concat([df_United_Kingdom, df_EUvsDis], ignore_index=True)\n",
        "\n",
        "#merge title and text\n",
        "df_Belgium_mix['text'] = df_Belgium_mix['title'] + '\\n\\n' + df_Belgium_mix['text']\n",
        "df_Cyprus_mix['text'] = df_Cyprus_mix['title'] + '\\n\\n' + df_Cyprus_mix['text']\n",
        "df_Ireland_mix['text'] = df_Ireland_mix['title'] + '\\n\\n' + df_Ireland_mix['text']\n",
        "df_Malta_mix['text'] = df_Malta_mix['title'] + '\\n\\n' + df_Malta_mix['text']\n",
        "df_United_Kingdom_mix['text'] = df_United_Kingdom_mix['title'] + '\\n\\n' + df_United_Kingdom_mix['text']\n",
        "\n",
        "#save\n",
        "df_Belgium_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_Belgium_EUvsDis_final.csv\", sep=',', index = False)\n",
        "df_Cyprus_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_Cyprus_EUvsDis_final.csv\", sep=',', index = False)\n",
        "df_Ireland_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_Ireland_EUvsDis_final.csv\", sep=',', index = False)\n",
        "df_Malta_mix.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_Malta_EUvsDis_final.csv\", sep=',', index = False)\n",
        "df_United_Kingdom.to_csv(\"/content/drive/MyDrive/DATASETS/FINAL_DATASETS/EU_FINAL_DATASETS/EU_mix_United_Kingdom_EUvsDis_final.csv\", sep=',', index = False)"
      ],
      "metadata": {
        "id": "BMFMqz1G-gh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOf71Fp7jOedERETcrGBAx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}